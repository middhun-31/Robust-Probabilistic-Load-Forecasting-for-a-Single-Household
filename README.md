# Robust Probabilistic Load Forecasting: A Comparative Study from SARIMA to Transformers on the REFIT Dataset
This repository contains the complete code, analysis, and paper for a deep-dive case study into probabilistic time-series forecasting. The project tackles the challenges of real-world, messy data using the REFIT dataset (House 1).The core of this project is a systematic, evidence-based approach to:Handling a large structural data gap through a rigorous imputation experiment.Evaluating a hierarchy of models, from classical baselines (SARIMA, Prophet) to modern machine learning (XGBoost, LightGBM) and deep learning (LSTM, Temporal Fusion Transformer).Moving beyond point forecasts (RMSE/MAE) to produce and evaluate robust probabilistic forecasts using Quantile Loss, Prediction Interval Coverage (PICP), and Average Quantile Score (AQS).This project was developed as a key portfolio piece for graduate school applications, demonstrating an end-to-end data science research workflow.ğŸš€ Key Results & FindingsThe analysis concludes that while classical models (SARIMA, Prophet) fundamentally fail to capture the data's non-linear, regime-switching behavior (weekdays vs. weekends), and tree-based models (XGBoost) provide a strong point forecast, the probabilistic deep learning models offer the most robust solution.The Temporal Fusion Transformer (TFT) emerged as the superior all-around model, achieving the best point forecast accuracy (lowest RMSE) and producing robust, dynamic uncertainty intervals that successfully captured the data's extreme volatility.Final Model ComparisonModelRMSEMAEPICP (90%)Avg. Quantile ScoreSeasonal NaÃ¯ve623.27327.65N/AN/ASARIMAX2815.972565.09N/AN/ALightGBM454.69258.10N/AN/AXGBoost452.92260.68N/AN/ALSTM (Prob.)517.47295.8688.81%84.01TFT (Prob.)481.94295.8995.85%84.95Probabilistic ForecastsThe final plots clearly show the superiority of the deep learning models. While the LSTM (left) was well-calibrated, the TFT (right) produced more cautious and dynamic intervals that better respected the data's volatility.LSTM Probabilistic ForecastTFT Probabilistic ForecastğŸ”¬ Methodology HighlightsImputation Experiment: Before modeling, we conducted a controlled experiment to select an imputation method for the multi-month data gap. We proved that a Seasonal Imputer (using the mean of the same hour and day-of-week) was essential for preserving the data's bimodal distribution, a task where standard Linear Imputation failed.Failure Analysis: We demonstrated why classical models like SARIMA and Prophet failed, showing their inability to model the sharp, non-linear drop in consumption during weekends.Feature Engineering: We implemented a robust feature engineering pipeline, including calendar features, a crucial is_weekend flag (to handle regime switching), and multiple lag features (lag_1hr, lag_24hr, lag_168hr) to capture autocorrelation.ğŸ“ Repository Structure.
â”œâ”€â”€ paper/
â”‚   â””â”€â”€ REFIT_Forecasting_Paper.pdf    # The final research paper
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_Data_Exploration_and_Imputation.ipynb
â”‚   â”œâ”€â”€ 02_Classical_and_ML_Models.ipynb
â”‚   â”œâ”€â”€ 03_Deep_Learning_LSTM.ipynb
â”‚   â”œâ”€â”€ 04_Deep_Learning_TFT.ipynb
â”‚   â””â”€â”€ 05_Final_Results_and_Plots.ipynb
â”œâ”€â”€ data/
â”‚   â””â”€â”€ (Data files are not uploaded due to size, but are available from the REFIT dataset)
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
